{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(file_in, file_out, train_words):\n",
    "    read = open(file_in, \"r\")    # Open the input file in read mode\n",
    "    \n",
    "    text = \"\"\n",
    "    \n",
    "    # Loop through each sentence in the input file, convert to lowercase and pad\n",
    "    for sentence in read.readlines():\n",
    "        text += \" <s> \" + sentence.lower() + \" </s> \"\n",
    "        \n",
    "   \n",
    "    text = text.split()         # Split the processed text into individual words\n",
    "    \n",
    "    words = {}\n",
    "    \n",
    "    # Count the frequency of each word in the text\n",
    "    for word in text:\n",
    "        if word in words:\n",
    "            words[word] += 1\n",
    "        else:\n",
    "            words[word] = 1\n",
    "            \n",
    "    # If train_words is None, replace single occurrence words with <unk>\n",
    "    if train_words is None:\n",
    "        for i in range(len(text)):\n",
    "            if words[text[i]] == 1:\n",
    "                text[i] = \"<unk>\"\n",
    "    else: \n",
    "        # If train_words is provided, replace unseen words with <unk>\n",
    "        for i in range(len(text)):\n",
    "            if text[i] not in train_words:\n",
    "                text[i] = \"<unk>\"\n",
    "                \n",
    "    # Count the frequency of <unk> words in the text\n",
    "    unk_words = {}\n",
    "    for word in text:\n",
    "        if word in unk_words:\n",
    "            unk_words[word] += 1\n",
    "        else:\n",
    "            unk_words[word] = 1\n",
    "    \n",
    "    out = open(file_out, \"w\")         # Open the output file in write mode\n",
    "    txt = ' '.join(text)              # Convert the list of words back to text\n",
    "    out.write(txt)                    # Write the processed text to the output file\n",
    "    out.close()\n",
    "    \n",
    "    return [words, unk_words, text]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the various txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "[train_count, train_unk_count, train_text] = preProcess(\"brown.train.txt\", \"trainOut.txt\", None)\n",
    "[test_count, test_unk_count, test_text] = preProcess(\"brown.test.txt\", \"testOut.txt\", train_unk_count)\n",
    "[dev_count, dev_unk_count, dev_text] = preProcess(\"brown.dev.txt\", \"devOut.txt\", train_unk_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words 24796\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique words ' + str(len(train_unk_count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word tokens 1018784\n"
     ]
    }
   ],
   "source": [
    "print('Number of word tokens ' + str(sum(train_unk_count.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram(train_dict):\n",
    "    unigram = train_dict.copy()\n",
    "    unigram.pop('<s>', None)\n",
    "    n = sum(unigram.values())\n",
    "    \n",
    "    unigram_result = {}\n",
    "    for word, count in unigram.items():\n",
    "        unigram_result[word] = count / n\n",
    "    \n",
    "    return unigram_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a count matrix from the training text based on bigram counts\n",
    "def count_matrix(train_text, word_freq_dict, add_one):\n",
    "    # Extract the unique vocabulary words from the word frequency dictionary\n",
    "    vocabulary = list(set(word_freq_dict))\n",
    "\n",
    "    # Generate bigrams from the training text\n",
    "    bigrams = generate_bigrams(train_text)\n",
    "    \n",
    "    # Count the occurrences of each bigram\n",
    "    bigram_counts = Counter(bigrams)\n",
    "\n",
    "    # Create column index dictionary for the vocabulary words\n",
    "    col_index = {}\n",
    "    for i, word in enumerate(vocabulary):\n",
    "        col_index[word] = i\n",
    "\n",
    "    # Create row index dictionary, which is a copy of the column index\n",
    "    row_index = col_index.copy()\n",
    "\n",
    "    # Get the number of rows and columns for the count matrix\n",
    "    nrow = len(row_index)\n",
    "    ncol = len(col_index)\n",
    "    \n",
    "    # Initialize a count matrix with zeros\n",
    "    count_matrix = np.zeros((nrow, ncol))\n",
    "\n",
    "    # Populate the count matrix with bigram counts\n",
    "    for bigram, count in bigram_counts.items():\n",
    "        prev_word, curr_word = bigram\n",
    "        i = row_index[prev_word]\n",
    "        j = col_index[curr_word]\n",
    "        count_matrix[i, j] = count\n",
    "\n",
    "    # Add-one smoothing if required\n",
    "    if add_one:\n",
    "        for i in range(nrow):\n",
    "            for j in range(ncol):\n",
    "                count_matrix[i, j] += 1\n",
    "\n",
    "    # Convert the count matrix into a DataFrame with row and column indices\n",
    "    count_matrix = pd.DataFrame(count_matrix, index=row_index, columns=vocabulary)\n",
    "    return count_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram(train_text, word_freq_dict, add_one_smoothing):\n",
    "    count_matrix = make_count_matrix(train_text, word_freq_dict, add_one_smoothing)\n",
    "    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n",
    "    return prob_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perc_words(train, test):\n",
    "    # Initialize variables to store counts and percentages\n",
    "    sum_tokens = 0\n",
    "    num_types = 0\n",
    "    test_tokens = sum(test.values())\n",
    "    test_type = len(test)\n",
    "    \n",
    "    # Count the number of unseen words in the test set\n",
    "    for word in test:\n",
    "        if word not in train:\n",
    "            sum_tokens += test[word]\n",
    "            num_types += 1\n",
    "            \n",
    "    # Calculate percentages\n",
    "    types_perc = num_types / test_type * 100\n",
    "    tokens_perc = sum_tokens / test_tokens * 100\n",
    "    \n",
    "    # Print the percentages\n",
    "    print('Percentage of word types ' + str(types_perc))\n",
    "    print('Percentage of word tokens ' + str(tokens_perc))\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of word types 15.623797922810617\n",
      "Percentage of word tokens 1.9957318074153982\n"
     ]
    }
   ],
   "source": [
    "perc_words(train_count, test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of word types 16.283174683225116\n",
      "Percentage of word tokens 2.0942205705880075\n"
     ]
    }
   ],
   "source": [
    "perc_words(train_count, dev_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = unigram(train_unk_count)\n",
    "bigram = bigram(train_text, train_unk_count, False)\n",
    "#bigram_add1 = bigram(train_text, train_unk_count, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1018784\n"
     ]
    }
   ],
   "source": [
    "print(sum(train_unk_count.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up scentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_symbol = '</s>'\n",
    "\n",
    "scentences = [\"He was laughed off the screen . \",\n",
    "             \"There was no compulsion behind them . \",\n",
    "             \"I look forward to hearing your reply . \"]\n",
    "\n",
    "i =0\n",
    "\n",
    "# Pad scentences\n",
    "scentences [0] = scentences[0].lower() + '</s>'\n",
    "scentences [1] = scentences[1].lower() + '</s>'\n",
    "scentences [2] = scentences[2].lower() + '</s>'\n",
    "\n",
    "\n",
    "# Add them to a list\n",
    "words = []\n",
    "for sentence in scentences:\n",
    "    for word in scentence.split():\n",
    "        words.append(word)\n",
    "        \n",
    "# <unk> words\n",
    "for i in range(len(words)):\n",
    "    if words[i] not in unigram:\n",
    "        words[i] = '<unk>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Log Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = {}\n",
    "for word in words:\n",
    "    subset[word] = unigram[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log probabilities of words\n",
    "log_prob = {}\n",
    "for word, count in subset.items():\n",
    "    log_prob[word] = math.log(count, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the log probabilities\n",
    "sum_log_prob = 0\n",
    "for word in words:\n",
    "    sum_log_prob += log_prob[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Log Probability -9.218361943610052\n"
     ]
    }
   ],
   "source": [
    "avg_log_prob = sum_log_prob/ len(words)\n",
    "print('Unigram Log Probability ' + str(sum_log_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram perplexity 433.71684018402664\n"
     ]
    }
   ],
   "source": [
    "perplexity = 2 ** (-avg_log_prob)\n",
    "print('Unigram perplexity ' + str(perplexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplex_uni(text, unigram):\n",
    "    # Create a subset dictionary to store log probabilities of each word in the text\n",
    "    subset = {}\n",
    "    \n",
    "    # Iterate over each word in the text\n",
    "    for word in text:\n",
    "        # Exclude the start symbol \"<s>\" from consideration\n",
    "        if word != \"<s>\":\n",
    "            # Calculate and store the log probability of the word using the unigram model\n",
    "            subset[word] = math.log(unigram[word], 2)\n",
    "\n",
    "    # Initialize variables to calculate the average log probability\n",
    "    sum_log_prob = 0 \n",
    "    m = 0  \n",
    "    \n",
    "    # Iterate over each word in the text, exclude symbols and accumulate log probs\n",
    "    for word in text:\n",
    "        if word != \"<s>\":\n",
    "            sum_log_prob += subset[word]\n",
    "            m += 1\n",
    "\n",
    "    # Calculate the average log probability\n",
    "    avg_log_prob = sum_log_prob / m\n",
    "    \n",
    "    # Calculate perplexity using the average log probability\n",
    "    perplexity = 2 ** (-avg_log_prob)\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test unigram perplexiy 696.3522405243207\n",
      "Dev unigram perplexiy 692.1274970283\n"
     ]
    }
   ],
   "source": [
    "print('Test unigram perplexiy ' + str(perplex_uni(test_text, unigram)))\n",
    "print('Dev unigram perplexiy ' + str(perplex_uni(dev_text, unigram)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Log Probability/Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(scentences)):\n",
    "    if scentences[i] not in bigram:\n",
    "        scentences[i] = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = set()\n",
    "sum_log_prob = 0\n",
    "has_zero = False\n",
    "\n",
    "# Iterate over the indices of the sentences list, excluding the last index\n",
    "for i in range(len(sentences) - 1):\n",
    "    # Add the current bigram (current word and the next word) along with its count to the subset set\n",
    "    subset.add((sentences[i], sentences[i + 1], bigram[sentences[i]][sentences[i + 1]]))\n",
    "    \n",
    "    # Check if the count of the current bigram is zero\n",
    "    if bigram[sentences[i]][sentences[i + 1]] == 0:\n",
    "        # Set the flag indicating that at least one bigram has a zero count\n",
    "        has_zero = True\n",
    "    else:\n",
    "        # Accumulate the log probability of the current bigram\n",
    "        sum_log_prob += math.log(bigram[sentences[i]][sentences[i + 1]], 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_prob = set()\n",
    "\n",
    "# Iterate over each bigram in the subset set\n",
    "for bigram in subset:\n",
    "    # Calculate the log probability of the current bigram and add it to the log_prob set\n",
    "    log_prob.add((bigram[0], bigram[1], math.log(bigram[2], 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(scentences)\n",
    "avg_log_prob = sum_log_prob / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram average Log Probability -9.218361943610052\n"
     ]
    }
   ],
   "source": [
    "print('Bigram average Log Probability ' + str(sum_log_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_perplexity = 2 ** -avg_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram perplexity 8.41397373458845\n"
     ]
    }
   ],
   "source": [
    "print('Bigram perplexity ' + str(bigram_perplexity))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
